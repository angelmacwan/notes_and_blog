<!DOCTYPE html> <html><head>
		<title>NLP</title>
		<base href="../">
		<meta id="root-path" root-path="../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="max - NLP">
		<meta property="og:title" content="NLP">
		<meta property="og:description" content="max - NLP">
		<meta property="og:type" content="website">
		<meta property="og:url" content="blog/nlp.html">
		<meta property="og:image" content="undefined">
		<meta property="og:site_name" content="max">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager theme-dark show-inline-title show-ribbon"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles"></style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="NLTK"><p dir="auto">NLTK</p></h1><div><p dir="auto"> <a href="?query=tag:classification" class="tag" target="_blank" rel="noopener">#classification</a> <a href="?query=tag:data" class="tag" target="_blank" rel="noopener">#data</a> <a href="?query=tag:language" class="tag" target="_blank" rel="noopener">#language</a> <a href="?query=tag:natural" class="tag" target="_blank" rel="noopener">#natural</a> <a href="?query=tag:naturel" class="tag" target="_blank" rel="noopener">#naturel</a> <a href="?query=tag:nlp" class="tag" target="_blank" rel="noopener">#nlp</a> <a href="?query=tag:notes" class="tag" target="_blank" rel="noopener">#notes</a> <a href="?query=tag:processing" class="tag" target="_blank" rel="noopener">#processing</a> <a href="?query=tag:sentence" class="tag" target="_blank" rel="noopener">#sentence</a> <a href="?query=tag:text" class="tag" target="_blank" rel="noopener">#text</a> <a href="?query=tag:token" class="tag" target="_blank" rel="noopener">#token</a> <a href="?query=tag:tokenization" class="tag" target="_blank" rel="noopener">#tokenization</a> <a href="?query=tag:words" class="tag" target="_blank" rel="noopener">#words</a></p></div><div class="heading-wrapper"><div class="heading-children"><div><p dir="auto"><code>pip3 install nltk</code></p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">import</span> nltk
nltk<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'all'</span><span class="token punctuation">)</span>
</code><button class="copy-code-button">Copy</button></pre></div><div><blockquote dir="auto">
<p>Corpora -&gt; Body of text<br>
lexicon -&gt; Words with meaning</p>
</blockquote></div></div></div><div class="heading-wrapper"><h1 data-heading="Tokenizing" dir="auto" class="heading" id="Tokenizing">Tokenizing</h1><div class="heading-children"><div><p dir="auto"><strong>sent_tokenize</strong> -&gt; Sentence Tokenizer, splits sentences in a body of text<br>
<strong>word_tokenize</strong> -&gt; Word Tokenizer, splits words in a sentence</p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> sent_tokenize<span class="token punctuation">,</span> word_tokenize

sampleText <span class="token operator">=</span> <span class="token string">"your text file"</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sent_tokenize<span class="token punctuation">(</span>sampletext<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>word_tokenize<span class="token punctuation">(</span>sampletext<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code><button class="copy-code-button">Copy</button></pre></div></div></div><div class="heading-wrapper"><h1 data-heading="Chunking" dir="auto" class="heading" id="Chunking">Chunking</h1><div class="heading-children"><div><p dir="auto">Grouping words in meaningful group</p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">import</span> nltk
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> state_union
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> PunktSentenceTokenizer

train_text <span class="token operator">=</span> state_union<span class="token punctuation">.</span>raw<span class="token punctuation">(</span><span class="token string">"2005-GWBush.txt"</span><span class="token punctuation">)</span>
txt <span class="token operator">=</span> <span class="token string">"Kids are playing. Kids like to play games. He got played"</span>

custTokenizer <span class="token operator">=</span> PunktSentenceTokenizer<span class="token punctuation">(</span>state_union<span class="token punctuation">.</span>raw<span class="token punctuation">(</span><span class="token string">"2006-GWBush.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
tokenizedText <span class="token operator">=</span> custTokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>txt<span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> tokenizedText<span class="token punctuation">:</span>
    words <span class="token operator">=</span> nltk<span class="token punctuation">.</span>word_tokenize<span class="token punctuation">(</span>i<span class="token punctuation">)</span>
    tag <span class="token operator">=</span> nltk<span class="token punctuation">.</span>pos_tag<span class="token punctuation">(</span>words<span class="token punctuation">)</span>

    chunkGram <span class="token operator">=</span> <span class="token triple-quoted-string string">r"""Chunk: {&lt;RB.?&gt;*&lt;VB.?&gt;*&lt;NNP&gt;+&lt;NN&gt;?}"""</span>
    chunkParser <span class="token operator">=</span> nltk<span class="token punctuation">.</span>RegexpParser<span class="token punctuation">(</span>chunkGram<span class="token punctuation">)</span>
    chunked <span class="token operator">=</span> chunkParser<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>tag<span class="token punctuation">)</span>
    <span class="token comment">#chunked.draw()</span>
    chunked<span class="token punctuation">.</span>pretty_print<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code><button class="copy-code-button">Copy</button></pre></div></div></div><div class="heading-wrapper"><h1 data-heading="Stop Words" dir="auto" class="heading" id="Stop_Words">Stop Words</h1><div class="heading-children"><div><p dir="auto">stop words are words that don't contribute a lot the text, or filler words like "a", "the"... this words are removed so that text can be made easier for machine to understand<br>
<code>stopwords.words("english")</code></p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> stopwords
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> word_tokenize

txt <span class="token operator">=</span> <span class="token string">"your text file / input"</span>

words <span class="token operator">=</span> word_tokenize<span class="token punctuation">(</span>txt<span class="token punctuation">)</span>

nw <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> words <span class="token keyword">if</span> i <span class="token keyword">not</span> <span class="token keyword">in</span> stopwords<span class="token punctuation">.</span>words<span class="token punctuation">(</span><span class="token string">"english"</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code><button class="copy-code-button">Copy</button></pre></div></div></div><div class="heading-wrapper"><h1 data-heading="Named Entity Recognition" dir="auto" class="heading" id="Named_Entity_Recognition">Named Entity Recognition</h1><div class="heading-children"><div><p dir="auto">The state_union data set will be used for training PunktSentenceTokenizer to create a custom tokenizer</p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">import</span> nltk
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> PunktSentenceTokenizer
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> state_union

txt <span class="token operator">=</span> <span class="token string">"Your text file or input"</span>

custTokenizer <span class="token operator">=</span> PunktSentenceTokenizer<span class="token punctuation">(</span>state_union<span class="token punctuation">.</span>raw<span class="token punctuation">(</span><span class="token string">"2006-GWBush.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
tokenizedText <span class="token operator">=</span> custTokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>txt<span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> tokenizedText<span class="token punctuation">:</span>
    words <span class="token operator">=</span> nltk<span class="token punctuation">.</span>word_tokenize<span class="token punctuation">(</span>i<span class="token punctuation">)</span>
    tag <span class="token operator">=</span> nltk<span class="token punctuation">.</span>pos_tag<span class="token punctuation">(</span>words<span class="token punctuation">)</span>
    nameEnt <span class="token operator">=</span> nltk<span class="token punctuation">.</span>ne_chunk<span class="token punctuation">(</span>tag<span class="token punctuation">)</span>
    nameEnt<span class="token punctuation">.</span>pretty_print<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code><button class="copy-code-button">Copy</button></pre></div></div></div><div class="heading-wrapper"><h1 data-heading="Stemming" dir="auto" class="heading" id="Stemming">Stemming</h1><div class="heading-children"><div><p dir="auto">Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. <code>ps.stem("word")</code></p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> nltk<span class="token punctuation">.</span>stem <span class="token keyword">import</span> PorterStemmer
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> word_tokenize

ps <span class="token operator">=</span> PorterStemmer<span class="token punctuation">(</span><span class="token punctuation">)</span>

txt <span class="token operator">=</span> <span class="token string">"Kids are playing. Kids like to play games. He got played"</span>
wt <span class="token operator">=</span> word_tokenize<span class="token punctuation">(</span>txt<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>ps<span class="token punctuation">.</span>stem<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> wt<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code><button class="copy-code-button">Copy</button></pre></div></div></div><div class="heading-wrapper"><h1 data-heading="Lemmatizing" dir="auto" class="heading" id="Lemmatizing">Lemmatizing</h1><div class="heading-children"><div><p dir="auto">A very similar operation to stemming is called lemmatizing. The major difference between these is, stemming can often create non-existent words, whereas lemmas are actual words.<br>
<code>pos = "a"</code> ⇒ Adjective<br>
<code>pos = "v"</code> ⇒ Verb</p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> nltk<span class="token punctuation">.</span>stem <span class="token keyword">import</span> WordNetLemmatizer

lemmatizer <span class="token operator">=</span> WordNetLemmatizer<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"cats"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"cacti"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"geese"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"rocks"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"python"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"better"</span><span class="token punctuation">,</span> pos<span class="token operator">=</span><span class="token string">"a"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"best"</span><span class="token punctuation">,</span> pos<span class="token operator">=</span><span class="token string">"a"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"run"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lemmatizer<span class="token punctuation">.</span>lemmatize<span class="token punctuation">(</span><span class="token string">"run"</span><span class="token punctuation">,</span><span class="token string">'v'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code><button class="copy-code-button">Copy</button></pre></div></div></div><div class="heading-wrapper"><h1 data-heading="Part of Speech Tagging" dir="auto" class="heading" id="Part_of_Speech_Tagging">Part of Speech Tagging</h1><div class="heading-children"><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">import</span> nltk
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> PunktSentenceTokenizer
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>corpus <span class="token keyword">import</span> state_union
<span class="token comment"># state_union data set will be used for training PunktSentenceTokenizer to create a custom tokenizer</span>

txt <span class="token operator">=</span> <span class="token string">"Kids are playing. Kids like to play games. He got played"</span>

custTokenizer <span class="token operator">=</span> PunktSentenceTokenizer<span class="token punctuation">(</span>state_union<span class="token punctuation">.</span>raw<span class="token punctuation">(</span><span class="token string">"2006-GWBush.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
tokenizedText <span class="token operator">=</span> custTokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>txt<span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> tokenizedText<span class="token punctuation">:</span>
    words <span class="token operator">=</span> nltk<span class="token punctuation">.</span>word_tokenize<span class="token punctuation">(</span>i<span class="token punctuation">)</span>
    tag <span class="token operator">=</span> nltk<span class="token punctuation">.</span>pos_tag<span class="token punctuation">(</span>words<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>tag<span class="token punctuation">)</span>
</code><button class="copy-code-button">Copy</button></pre></div><div><hr></div></div></div><div class="heading-wrapper"><h1 data-heading="Tokenizing" dir="auto" class="heading" id="Tokenizing">Tokenizing</h1><div class="heading-children"><div><p dir="auto">Rather then letters, words are encoded into numbers because different words can have same letters in them<br>
<code>Tokenizer(num_words = 100)</code><br>
in above code, <code>num_words</code> refer to the maximum number of words to keep, if we have a huge text body, we can keep only top 100 most used words</p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> tensorflow <span class="token keyword">import</span> keras
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer

sentences <span class="token operator">=</span> <span class="token punctuation">[</span> <span class="token string">"I love my dog"</span><span class="token punctuation">,</span> <span class="token string">"I love my cat"</span> <span class="token punctuation">]</span>

tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">)</span> <span class="token comment"># Creating the model object</span>
tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span> <span class="token comment"># Training the model</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">)</span> <span class="token comment"># prints tokenized dictionary</span>
<span class="token comment"># output -&gt; {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}</span>

</code><button class="copy-code-button">Copy</button></pre></div><div class="heading-wrapper"><h2 data-heading="Tokenizing Sentences into numbers" dir="auto" class="heading" id="Tokenizing_Sentences_into_numbers"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Tokenizing Sentences into numbers</h2><div class="heading-children"><div><p dir="auto">if there is an unknown word that is passed in <code>tokenizer.texts_to_sequences()</code> method, that word is ignored.<br>
to deal with this, <code>oov_token</code> property is passed to <code>Tokenizer()</code> object, so whenever a new word is faced, it will be replaced by whatever is passed as argument to <code>oov_token</code> in this case it is <code>&lt;OOV&gt;</code> as it is unlikely to appear naturally in a body of text.</p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer

sentences <span class="token operator">=</span> <span class="token punctuation">[</span> <span class="token string">"I love my dog"</span><span class="token punctuation">,</span> <span class="token string">"I love my cat"</span><span class="token punctuation">,</span> <span class="token string">"you love my dog"</span><span class="token punctuation">,</span> <span class="token string">"Do you think my dog is amazing? "</span> <span class="token punctuation">]</span>

tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> oov_token<span class="token operator">=</span><span class="token string">"&lt;OOV&gt;"</span><span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>
word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index

<span class="token comment"># passing array of sentences to convert to ints</span>
seq <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token string">"He likes my cat"</span><span class="token punctuation">,</span> <span class="token string">"My dog is amazing!"</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span>
<span class="token comment"># [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]</span>

</code><button class="copy-code-button">Copy</button></pre></div></div></div></div></div><div class="heading-wrapper"><h1 data-heading="Padding" dir="auto" class="heading" id="Padding">Padding</h1><div class="heading-children"><div><p dir="auto">Text input in a mode can be of different sizes, to deal with this Padding is used.<br>
function <code>pad_sequence()</code> is used, this method will set the length of all your inputs to the longest string by adding zeros either in front or at end of the sequence<br>
<code>paddedSeq()</code> adds zeros in beginning<br>
<code>paddedSeq(padding='post')</code> adds zeros at the end<br>
<code>paddedSeq(seqData, padding='post', maxlen=10, truncating='post')</code></p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences

sentences <span class="token operator">=</span> <span class="token punctuation">[</span> <span class="token string">"I love my dog"</span><span class="token punctuation">,</span> <span class="token string">"I love my cat"</span><span class="token punctuation">,</span> <span class="token string">"you love my dog"</span><span class="token punctuation">,</span> <span class="token string">"Do you think my dog is amazing? "</span> <span class="token punctuation">]</span>

tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> oov_token<span class="token operator">=</span><span class="token string">"&lt;OOV&gt;"</span><span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>
word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index

<span class="token comment"># passing array of sentences to convert to ints</span>
seq <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token string">"He likes my cat"</span><span class="token punctuation">,</span> <span class="token string">"My dog is amazing!"</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>
paddedSeq <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>seq<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>paddedSeq<span class="token punctuation">)</span>

</code><button class="copy-code-button">Copy</button></pre></div><div><hr></div></div></div><div class="heading-wrapper"><h1 data-heading="Text vectorization" dir="auto" class="heading" id="Text_vectorization">Text vectorization</h1><div class="heading-children"><div><p dir="auto">Textual data is converted into numerical vectors that machine learning algorithms can understand.</p></div><div class="heading-wrapper"><h2 data-heading="Bag of Words (BoW)" dir="auto" class="heading" id="Bag_of_Words_(BoW)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Bag of Words (BoW)</h2><div class="heading-children"><div><ul>
<li data-line="0" dir="auto">BoW represents text as a collection of unique words along with their frequencies in the document</li>
<li data-line="1" dir="auto">Each document is represented as a vector, where each dimension corresponds to a unique word, and the value represents the frequency of that word in the document.</li>
</ul></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer

corpus <span class="token operator">=</span> <span class="token punctuation">[</span>
	<span class="token string">"This is the first document."</span><span class="token punctuation">,</span>
	<span class="token string">"This document is the second document."</span><span class="token punctuation">,</span>
	<span class="token string">"And this is the third one."</span><span class="token punctuation">,</span>
	<span class="token string">"Is this the first document?"</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

vectorizer <span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>vectorizer<span class="token punctuation">.</span>get_feature_names_out<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

</code><button class="copy-code-button">Copy</button></pre></div></div></div><div class="heading-wrapper"><h2 data-heading="Term Frequency-Inverse Document Frequency (TF-IDF)" dir="auto" class="heading" id="Term_Frequency-Inverse_Document_Frequency_(TF-IDF)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Term Frequency-Inverse Document Frequency (TF-IDF)</h2><div class="heading-children"><div><p dir="auto">TF-IDF stands for <em>Term Frequency-Inverse Document Frequency</em>. It's a numerical statistic used in information retrieval and text mining to evaluate the importance of a word in a document relative to a collection of documents. Here's a breakdown of what it entails:</p></div><div><ol>
<li data-line="0" dir="auto"><strong>Term Frequency (TF)</strong>: This measures the frequency of a term (word) within a document. It's simply the number of times a term appears in a document divided by the total number of terms in the document. It aims to represent how often a term occurs within a document relative to the total number of terms.</li>
<li data-line="1" dir="auto"><strong>Inverse Document Frequency (IDF)</strong>: This measures how important a term is across the entire corpus of documents. It's calculated by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. It helps in determining the weight of rare terms that may be more significant than common terms.</li>
<li data-line="2" dir="auto"><strong>TF-IDF</strong>: This combines TF and IDF to give a weight to each term in a document. The higher the TF-IDF score of a term in a document, the more important that term is to the document.<br>
The TF-IDF score is calculated for each term in each document in the corpus. It's commonly used in various natural language processing tasks such as document classification, information retrieval, and text mining to determine the importance of words in a document relative to a collection of documents.</li>
</ol></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfVectorizer

corpus <span class="token operator">=</span> <span class="token punctuation">[</span>
	<span class="token string">"This is the first document."</span><span class="token punctuation">,</span>
	<span class="token string">"This document is the second document."</span><span class="token punctuation">,</span>
	<span class="token string">"And this is the third one."</span><span class="token punctuation">,</span>
	<span class="token string">"Is this the first document?"</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

vectorizer <span class="token operator">=</span> TfidfVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>vectorizer<span class="token punctuation">.</span>get_feature_names_out<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

</code><button class="copy-code-button">Copy</button></pre></div></div></div><div class="heading-wrapper"><h2 data-heading="Word Embeddings (e.g., Word2Vec, GloVe)" dir="auto" class="heading" id="Word_Embeddings_(e.g.,_Word2Vec,_GloVe)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Word Embeddings (e.g., Word2Vec, GloVe)</h2><div class="heading-children"><div><p dir="auto">Word embeddings are a type of word representation in natural language processing (NLP) that captures the semantic meaning of words by mapping them to dense vectors in a continuous vector space.<br>
There are several popular algorithms for generating word embeddings, including Word2Vec, GloVe, and fastText.</p></div><div class="heading-wrapper"><h3 data-heading="**Word2Vec**" dir="auto" class="heading" id="**Word2Vec**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>Word2Vec</strong></h3><div class="heading-children"><div><p dir="auto">Word2Vec learns distributed representations of words by training a neural network on a large corpus of text. The key idea behind Word2Vec is the distributional hypothesis, which posits that words appearing in similar contexts tend to have similar meanings.</p></div><div><pre class="language-python" tabindex="0"><code class="language-python is-loaded"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> Word2Vec
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>utils <span class="token keyword">import</span> simple_preprocess

<span class="token comment"># Sample corpus</span>
corpus <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"This is the first document."</span><span class="token punctuation">,</span>
    <span class="token string">"This document is the second document."</span><span class="token punctuation">,</span>
    <span class="token string">"And this is the third one."</span><span class="token punctuation">,</span>
    <span class="token string">"Is this the first document?"</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

<span class="token comment"># Tokenize the corpus</span>
tokenized_corpus <span class="token operator">=</span> <span class="token punctuation">[</span>simple_preprocess<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token keyword">for</span> text <span class="token keyword">in</span> corpus<span class="token punctuation">]</span>

<span class="token comment"># Train Word2Vec model</span>
model <span class="token operator">=</span> Word2Vec<span class="token punctuation">(</span>tokenized_corpus<span class="token punctuation">,</span> vector_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> window<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>

<span class="token comment"># Get word vectors</span>
word_vectors <span class="token operator">=</span> model<span class="token punctuation">.</span>wv

<span class="token comment"># Find similar words</span>
similar_words <span class="token operator">=</span> word_vectors<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token string">"document"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Similar words to 'document':"</span><span class="token punctuation">,</span> similar_words<span class="token punctuation">)</span>

<span class="token comment"># Get vector for a specific word</span>
vector_for_word <span class="token operator">=</span> word_vectors<span class="token punctuation">[</span><span class="token string">"document"</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Vector for 'document':"</span><span class="token punctuation">,</span> vector_for_word<span class="token punctuation">)</span>

</code><button class="copy-code-button">Copy</button></pre></div><div><p dir="auto">In this example, we tokenize the corpus into lists of words using <code>simple_preprocess</code> from Gensim. Then, we train a Word2Vec model with a vector size of 100, a window size of 5 (maximum distance between the current and predicted word within a sentence), and a minimum word count of 1. After training, we can access word vectors using the <code>wv</code> attribute of the model. Finally, we can find similar words or retrieve the vector representation of a specific word.</p></div><div class="mod-footer"></div></div></div></div></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#NLTK"><div class="tree-item-contents heading-link" heading-name="NLTK"><span class="tree-item-title">NLTK</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Tokenizing"><div class="tree-item-contents heading-link" heading-name="Tokenizing"><span class="tree-item-title">Tokenizing</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Chunking"><div class="tree-item-contents heading-link" heading-name="Chunking"><span class="tree-item-title">Chunking</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Stop_Words"><div class="tree-item-contents heading-link" heading-name="Stop Words"><span class="tree-item-title">Stop Words</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Named_Entity_Recognition"><div class="tree-item-contents heading-link" heading-name="Named Entity Recognition"><span class="tree-item-title">Named Entity Recognition</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Stemming"><div class="tree-item-contents heading-link" heading-name="Stemming"><span class="tree-item-title">Stemming</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Lemmatizing"><div class="tree-item-contents heading-link" heading-name="Lemmatizing"><span class="tree-item-title">Lemmatizing</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Part_of_Speech_Tagging"><div class="tree-item-contents heading-link" heading-name="Part of Speech Tagging"><span class="tree-item-title">Part of Speech Tagging</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Tokenizing"><div class="tree-item-contents heading-link" heading-name="Tokenizing"><span class="tree-item-title">Tokenizing</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="blog\nlp.html#Tokenizing_Sentences_into_numbers"><div class="tree-item-contents heading-link" heading-name="Tokenizing Sentences into numbers"><span class="tree-item-title">Tokenizing Sentences into numbers</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Padding"><div class="tree-item-contents heading-link" heading-name="Padding"><span class="tree-item-title">Padding</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="blog\nlp.html#Text_vectorization"><div class="tree-item-contents heading-link" heading-name="Text vectorization"><span class="tree-item-title">Text vectorization</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="blog\nlp.html#Bag_of_Words_(BoW)"><div class="tree-item-contents heading-link" heading-name="Bag of Words (BoW)"><span class="tree-item-title">Bag of Words (BoW)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="blog\nlp.html#Term_Frequency-Inverse_Document_Frequency_(TF-IDF)"><div class="tree-item-contents heading-link" heading-name="Term Frequency-Inverse Document Frequency (TF-IDF)"><span class="tree-item-title">Term Frequency-Inverse Document Frequency (TF-IDF)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="blog\nlp.html#Word_Embeddings_(e.g.,_Word2Vec,_GloVe)"><div class="tree-item-contents heading-link" heading-name="Word Embeddings (e.g., Word2Vec, GloVe)"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Word Embeddings (e.g., Word2Vec, GloVe)</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="blog\nlp.html#**Word2Vec**"><div class="tree-item-contents heading-link" heading-name="**Word2Vec**"><span class="tree-item-title"><strong>Word2Vec</strong></span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>